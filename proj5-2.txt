1)
    Run your code for both SimplePageRank and BackedgesPageRank for 20
iterations on both Facebook dataset and the Enron dataset, parititioned into
chunks of 500, on clusters of size 5 and 10. How long do each of these 8
scenarios take?

**************************  Performance in 8 (36) Scenarios  *************************
* Instances    | Mode: S FaceBook | Mode: B Facebook | Mode S: Enron | Mode B: Enron *
*     5              65.43 min         67.14 min        69.35 min       75.55 min    *
*    10              10.11 min         11.14 min        11.40 min       12.58 min    *
*------------------------------------------------------------------------------------*
*    10   (1 part.)   2.06 min         11.37 min         1.63 min        3.30 min    *
*    10   (2 part.)   1.27 min          4.52 min         1.11 min        1.69 min    *
*    10   (5 part.)   0.71 min          1.57 min         0.67 min        0.83 min    *
*    10  (10 part.)   0.67 min          0.89 min         0.46 min        0.56 min    *
*    10  (20 part.)   0.59 min          0.61 min         0.47 min        0.48 min    *
*    10  (50 part.)   0.87 min          0.89 min         0.76 min        0.77 min    *
*    10 (100 part.)   1.87 min          1.38 min         1.22 min        1.25 min    *
**************************************************************************************

2)
    When running SimplePageRank on the 10 instances with a repartition count of
500, what was the ratio of size of the input file to the runtime of your
program for the Enron dataset? How about the Facebook dataset? Does this match
your expectations?

Facebook file size: 854362 Bytes = 0.85 MB
Enron file size:    4049333 Bytes = 4.05 MB

FB file size / SimplePageRank run-time (minutes) = 0.0841
Enron file size / SimplePageRank run-time (minutes) = 0.3553

The above ratios are bytes per minute; it makes sense that the 4x larger dataset,
Enron, has byte/minute ratio almost 4x higher than in the case with the
Facebook dataset. This result shows that the EC2 servers are making effective
use of Mapreduce's performance benefits.

3)
   What was the speedup for 10 instances relative to 5 instances for 20 iterations
of BackedgesPageRank on the Enron dataset with a repartition count of 500? What
do you conclude about how well Spark parallelizes your work? How does the
algorithm scale in regards to strong scaling? weak scaling?

- 6x speed up
- Strong scaling, weak scaling means increasing the problem size, which we're not doing

4)
    In part 5, you tinkered with the repartition count. At what repartition
count was your code the fastest on average? Why do you think it would go slower
if you decreased the partition count? Why do you think it would go slower if
you increased the partition count?

- Fastest performance was with 20 partitions
- 10 instances (each with 2 cores) means there can be 20 partitions of data
  being processed in a parallel fashion. Any more or fewer partitions results
  in a lost of performance.

5)
    How many dollars in EC2 credits did you use to complete this project?
Remember the price of single c1.medium machine is $0.0161 per Hour, and a
cluster with 10 slaves has 11 machines (the master counts as one).

**********  Variables  ***********
* P = $0.0161 (dollars per hour) *
* C = 10.1195 (hours for certain *
*               recorded below ) *
* W = 82.1195 (WORST CASE **)    *
**********************************

** Worst case is if I accidently stayed logged in over the course of a few days
   at the beginning of the project -- I didn't succeed with make launch-test so
   I don't think this is the case...

***  Recorded Time Running Tasks  ***
*  Pre-part 4:          240    min  *
*  part 4 w/ 500:       277.47 min  *
*  part 5 w/ 500:       45.23  min  *
*  part 5 w/ 1:         18.36  min  *
*  part 5 w/ 2:         8.59   min  *
*  part 5 w/ 5:         3.78   min  *
*  part 5 w/ 10:        2.58   min  *
*  part 5 w/ 20:        2.15   min  *
*  part 5 w/ 50:        3.29   min  *
*  part 5 w/ 100:       5.72   min  *
*************************************

Cost running 1 instance   = P *    (4 hours)   * 1 machine   + master  = $0.1288
Cost running 5 instances  = P * (4.6245 hours) * 5 machines  + master  = $0.4467267
Cost running 10 instances = P * (1.495 hours)  * 10 machines + master  = $0.2647645

Cost using recorded time: Sum of costs of per # of instances    = $0.8402912
** Using worst case time on 1 instances (sub 4 hours for 76 hours) = $3.1586912
